{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSXQ9Iz0wPr_"
      },
      "source": [
        "import matplotlib.pyplot as plt # plotting library\n",
        "import numpy as np # this module is useful to work with numerical arrays\n",
        "import pandas as pd \n",
        "import random \n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "data_dir = 'dataset'\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n",
        "test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset.transform = train_transform\n",
        "test_dataset.transform = test_transform\n",
        "\n",
        "m=len(train_dataset)\n",
        "\n",
        "train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n",
        "batch_size=256\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
        "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4gHBa1PwhnY"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        ### Convolutional section\n",
        "        self.encoder_cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        ### Flatten layer\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "### Linear section\n",
        "        self.encoder_lin = nn.Sequential(\n",
        "            nn.Linear(3 * 3 * 32, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, encoded_space_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder_cnn(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.encoder_lin(x)\n",
        "        return x\n",
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
        "        super().__init__()\n",
        "        self.decoder_lin = nn.Sequential(\n",
        "            nn.Linear(encoded_space_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 3 * 3 * 32),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.unflatten = nn.Unflatten(dim=1, \n",
        "        unflattened_size=(32, 3, 3))\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, 3, \n",
        "            stride=2, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
        "            padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
        "            padding=1, output_padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.decoder_lin(x)\n",
        "        x = self.unflatten(x)\n",
        "        x = self.decoder_conv(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgWkcmMJwoaJ",
        "outputId": "144f62ec-227b-439c-b3bd-280a1f31ca61"
      },
      "source": [
        "\n",
        "### Define the loss function\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "### Define an optimizer (both for the encoder and the decoder!)\n",
        "lr= 0.001\n",
        "\n",
        "### Set the random seed for reproducible results\n",
        "torch.manual_seed(0)\n",
        "\n",
        "### Initialize the two networks\n",
        "d = 4\n",
        "\n",
        "#model = Autoencoder(encoded_space_dim=encoded_space_dim)\n",
        "encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n",
        "decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)\n",
        "params_to_optimize = [\n",
        "    {'params': encoder.parameters()},\n",
        "    {'params': decoder.parameters()}\n",
        "]\n",
        "\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=lr, weight_decay=1e-05)\n",
        "\n",
        "# Check if the GPU is available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f'Selected device: {device}')\n",
        "\n",
        "# Move both the encoder and the decoder to the selected device\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "decoder.to(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected device: cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (decoder_lin): Sequential(\n",
              "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Linear(in_features=128, out_features=288, bias=True)\n",
              "    (3): ReLU(inplace=True)\n",
              "  )\n",
              "  (unflatten): Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
              "  (decoder_conv): Sequential(\n",
              "    (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c24XWpEEwx42"
      },
      "source": [
        "### Training function\n",
        "def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer,noise_factor=0.3):\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    train_loss = []\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
        "        # Move tensor to the proper device\n",
        "        image_noisy = .add_noise(image_batch,noise_factor)\n",
        "        image_noisy = image_noisy.to(device)    \n",
        "        # Encode data\n",
        "        encoded_data = encoder(image_noisy)\n",
        "        # Decode data\n",
        "        decoded_data = decoder(encoded_data)\n",
        "        # Evaluate loss\n",
        "        loss = loss_fn(decoded_data, image_noisy)\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print batch loss\n",
        "        print('\\t partial train loss (single batch): %f' % (loss.data))\n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    return np.mean(train_loss)\n",
        "\n",
        "def train_epoch_den(encoder, decoder, device, dataloader, loss_fn, optimizer,noise_factor=0.3):\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    train_loss = []\n",
        "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
        "    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
        "        # Move tensor to the proper device\n",
        "        image_noisy = mne.simulation.add_noise(image_batch,noise_factor)\n",
        "        image_noisy = image_noisy.to(device)    \n",
        "        # Encode data\n",
        "        encoded_data = encoder(image_noisy)\n",
        "        # Decode data\n",
        "        decoded_data = decoder(encoded_data)\n",
        "        # Evaluate loss\n",
        "        loss = loss_fn(decoded_data, image_noisy)\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print batch loss\n",
        "        print('\\t partial train loss (single batch): %f' % (loss.data))\n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "\n",
        "    return np.mean(train_loss)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWlqXHNuw67e"
      },
      "source": [
        "### Testing function\n",
        "def test_epoch(encoder, decoder, device, dataloader, loss_fn):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        # Define the lists to store the outputs for each batch\n",
        "        conc_out = []\n",
        "        conc_label = []\n",
        "        for image_batch, _ in dataloader:\n",
        "            # Move tensor to the proper device\n",
        "            image_batch = image_batch.to(device)\n",
        "            # Encode data\n",
        "            encoded_data = encoder(image_batch)\n",
        "            # Decode data\n",
        "            decoded_data = decoder(encoded_data)\n",
        "            conc_out.append(decoded_data.cpu())\n",
        "            conc_label.append(image_batch.cpu())\n",
        "        # Create a single tensor with all the values in the lists\n",
        "        conc_out = torch.cat(conc_out)\n",
        "        conc_label = torch.cat(conc_label) \n",
        "        # Evaluate global loss\n",
        "        val_loss = loss_fn(conc_out, conc_label)\n",
        "    return val_loss.data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AMSMlYexAz-"
      },
      "source": [
        "\n",
        "def plot_ae_outputs(encoder,decoder,n=5):\n",
        "    plt.figure(figsize=(10,4.5))\n",
        "    for i in range(n):\n",
        "      ax = plt.subplot(2,n,i+1)\n",
        "      img = test_dataset[i][0].unsqueeze(0).to(device)\n",
        "      encoder.eval()\n",
        "      decoder.eval()\n",
        "      with torch.no_grad():\n",
        "         rec_img  = decoder(encoder(img))\n",
        "      plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)  \n",
        "      if i == n//2:\n",
        "        ax.set_title('Original images')\n",
        "      ax = plt.subplot(2, n, i + 1 + n)\n",
        "      plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)  \n",
        "      if i == n//2:\n",
        "         ax.set_title('Reconstructed images')\n",
        "    plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "g1Bn0kW6xKVI",
        "outputId": "28710b7b-5243-4e96-f942-03c048bdaf1c"
      },
      "source": [
        "def add_noise(cor, epsilon=None, m=None):\n",
        "    if isinstance(cor, pd.DataFrame):\n",
        "        cor = cor.values\n",
        "    elif isinstance(cor, np.ndarray) is False:\n",
        "        cor = np.array(cor)\n",
        "\n",
        "    n = cor.shape[1]\n",
        "\n",
        "    if epsilon is None:\n",
        "        epsilon = 0.05\n",
        "    if m is None:\n",
        "        m = 2\n",
        "\n",
        "    np.fill_diagonal(cor, 1 - epsilon)\n",
        "\n",
        "    cor = SimulateCorrelationMatrix._generate_noise(cor, n, m, epsilon)\n",
        "\n",
        "    return cor \n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "diz_loss = {'train_loss':[],'val_loss':[]}\n",
        "for epoch in range(num_epochs):\n",
        "   train_loss =train_epoch(encoder,decoder,device,\n",
        "   train_loader,loss_fn,optim)\n",
        "   val_loss = test_epoch(encoder,decoder,device,test_loader,loss_fn)\n",
        "   print('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "   diz_loss['train_loss'].append(train_loss)\n",
        "   diz_loss['val_loss'].append(val_loss)\n",
        "   plot_ae_outputs(encoder,decoder,n=5)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-85d4266993d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m    train_loss =train_epoch(encoder,decoder,device,\n\u001b[0;32m---> 25\u001b[0;31m    train_loader,loss_fn,optim)\n\u001b[0m\u001b[1;32m     26\u001b[0m    \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-5b6ffeebabb8>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, device, dataloader, loss_fn, optimizer, noise_factor)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# with \"_\" we just ignore the labels (the second element of the dataloader tuple)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Move tensor to the proper device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mimage_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoise_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mimage_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_noisy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Encode data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-85d4266993d6>\u001b[0m in \u001b[0;36madd_noise\u001b[0;34m(cor, epsilon, m)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_diagonal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulateCorrelationMatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mfill_diagonal\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/index_tricks.py\u001b[0m in \u001b[0;36mfill_diagonal\u001b[0;34m(a, val, wrap)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m# all dimensions equal, so we check first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0malltrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All dimensions of input must be of equal length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcumprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All dimensions of input must be of equal length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "OspsGpM7xRNV",
        "outputId": "655377a8-dd22-4e44-d8b2-cf40b3f2c2b9"
      },
      "source": [
        "# Plot losses\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.semilogy(diz_loss['train_loss'], label='Train')\n",
        "plt.semilogy(diz_loss['val_loss'], label='Valid')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Loss')\n",
        "#plt.grid()\n",
        "plt.legend()\n",
        "#plt.title('loss')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5cdhnV8xUoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "e84c8ec7-58a1-4328-8aff-a6d56e1fc934"
      },
      "source": [
        "\n",
        "def plot_reconstructed(decoder, r0=(-5, 10), r1=(-10, 5), n=12):\n",
        "    plt.figure(figsize=(20,8.5))\n",
        "    w = 28\n",
        "    img = np.zeros((n*w, n*w))\n",
        "    for i, y in enumerate(np.linspace(*r1, n)):\n",
        "        for j, x in enumerate(np.linspace(*r0, n)):\n",
        "            z = torch.Tensor([[x, y]]).to(device)\n",
        "            x_hat = decoder(z)\n",
        "            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n",
        "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
        "    plt.imshow(img, extent=[*r0, *r1], cmap='gist_gray')\n",
        "    \n",
        "plot_reconstructed(decoder, r0=(-1, 1), r1=(-1, 1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-6e6d2e165d5d>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    plot_reconstructed(decoder, r0=(-1, 1), r1=(-1, 1)\u001b[0m\n\u001b[0m                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnm3P5J5xm3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "4e050d4c-b919-4095-ce68-d2fa970235ac"
      },
      "source": [
        "encoded_samples = []\n",
        "for sample in tqdm(test_dataset):\n",
        "    img = sample[0].unsqueeze(0).to(device)\n",
        "    label = sample[1]\n",
        "    # Encode image\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded_img  = encoder(img)\n",
        "    # Append to list\n",
        "    encoded_img = encoded_img.flatten().cpu().numpy()\n",
        "    encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
        "    encoded_sample['label'] = label\n",
        "    encoded_samples.append(encoded_sample)\n",
        "encoded_samples = pd.DataFrame(encoded_samples)\n",
        "encoded_samples"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e67ec9677063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoded_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Encode image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-w3h7Hmxs99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a3e4a950-0d6f-4e2f-e00a-61693287d1d4"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', \n",
        "           color=encoded_samples.label.astype(str), opacity=0.7)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-c07ad18e15e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m px.scatter(encoded_samples, x='Enc. Variable 0', y='Enc. Variable 1', \n\u001b[0m\u001b[1;32m      4\u001b[0m            color=encoded_samples.label.astype(str), opacity=0.7)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoded_samples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju9Bonyfxsnc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "2251172b-44c2-45d2-acb6-eef86f1c7d60"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_results = tsne.fit_transform(encoded_samples.drop(['label'],axis=1))\n",
        "fig = px.scatter(tsne_results, x=0, y=1,\n",
        "                 color=encoded_samples.label.astype(str),\n",
        "                 labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n",
        "fig.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-44f6f7f6d9e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtsne_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m fig = px.scatter(tsne_results, x=0, y=1,\n\u001b[1;32m      6\u001b[0m                  \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'drop'"
          ]
        }
      ]
    }
  ]
}